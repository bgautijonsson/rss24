---
title: "Gaussian Copulas for Large Spatial Fields"
subtitle: "Modeling Data-Level Spatial Dependence in Multivariate Generalized Extreme Value Distributions"
author: "Brynjólfur Gauti Guðrúnar Jónsson"
institute: "University of Iceland"
format: 
  revealjs:
    theme: theme.scss
    auto-stretch: true
    view-distance: 5
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar mb-10'><ul class='menu'></ul><div>"
        footer: "<div class='footer footer-default' style='display: block;'> <a href='https://bggj.is/rss24' target='_blank'>bggj.is/rss24</a>  <img src='images/hi-audkenni_28-raunsvisd.png' class='slide-logo r-stretch'></div>"
      scale: 0.6
      width: 1600
      height: 900
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
revealjs-plugins:
  - simplemenu
---

```{r setup}
library(stdmatern)
library(INLA)
library(gt)
options(width = 648)
```

## Introduction {data-name="Background"}
::: {style="font-size:70%"}
- Case study: UKCP Local Projections on a 5km grid over the UK (1980-2080)
- Challenge: Modeling maximum daily precipitation in yearly blocks
  - 43,920 spatial locations on a 180 x 244 grid
- Computational complexity: Fitting GEV distributions to all locations
- Two aspects of spatial dependence:
  1. GEV parameters (GMRF models)
  2. Data-level dependence (Copulas)

:::

## Calculating Multivariate Normal Densities

::: {.columns style="font-size:50%"}

::: {.column width="50%"}
### Log Density Formula

$$
\log f(\mathbf{x}) \propto \frac{1}{2}\left(\log |\mathbf{Q}| - \mathbf{x}^T\mathbf{Q}\mathbf{x}\right)
$$

### Key Components

1. **Log Determinant**: $\log |\mathbf{Q}|$
   - Constant for a given precision matrix
2. **Quadratic Form**: $\mathbf{x}^T\mathbf{Q}\mathbf{x}$
   - Needs calculation for each density evaluation
   - Represents Mahalanobis distance
:::

::: {.column width="50%"}
### Computational Challenges

- Log determinant calculation
  - Time complexity: $O(n^3)$ for naive methods
  - Memory complexity: $O(n^2)$
- Quadratic form calculation
  - Time complexity: $O(n^2)$
  - Critical for performance in large spatial fields

### Spatial Model Considerations

- Some models (e.g., ICAR) avoid log determinant calculation
- Efficient computation crucial for large-scale applications
:::

:::

## Spatial Models

::: {style="font-size:40%"}

::: {.columns}
#### Conditional Autoregression (CAR)
::: {.column width="50%"}

* $\mathbf{D}$ is a diagonal matrix with $D_{ii} = n_i$, the number of neighbours of $i$
* $\mathbf{A}$ is the adjacency matrix with $A_{ij} = A_{ji} = 1$ if $i \sim j$
* $\tau$ models overall precision
:::
::: {.column width="50%"}
$$
\begin{aligned}
\mathbf{x} &\sim N(\mathbf{0}, \tau \mathbf{Q}) \\
\mathbf{Q} &= \mathbf{D}\left(\mathbf{I} - \alpha \mathbf{A} \right)
\end{aligned}
$$
:::
::: 

----------

::: {.columns}
#### Besag's Intrinsic Conditional Autoregression (ICAR)
::: {.column width="50%"}

* $\alpha = 1$, so $\mathbf Q$ is singular, but constant
* Don't have to calculate $\log |\mathbf{Q}|$
* $\tau$ is a precision parameter
:::
::: {.column width="50%"}
$$
\begin{aligned}
\mathbf{x} &\sim N(\mathbf{0}, \tau \mathbf{Q}) \\
\mathbf{Q} &= \mathbf{D} - \mathbf{W}
\end{aligned}
$$
:::
:::

----------

::: {.columns}
#### BYM (Besag-York-Mollié) Model
::: {.column width="50%"}

* $\mathbf{u}$ is the structured spatial component (Besag model)
* $\mathbf{v}$ is the unstructured component (i.i.d. normal)
* $\tau_u$ and $\tau_v$ are precision parameters for each component
:::
::: {.column width="50%"}
$$
\begin{aligned}
\mathbf{x} &= \mathbf{u} + \mathbf{v} \\
\mathbf{u} &\sim \mathrm{ICAR}(\tau_u) \\
\mathbf{v} &\sim N(\mathbf{0}, \tau_v^{-1})
\end{aligned}
$$
:::
:::

----------

::: {.columns}
#### BYM2  Model
::: {.column width="50%"}

* Rewrite the combination to get proper scaling
* $\rho$ models how much of variance is spatial
* $s$ is a scaling factor chosen to make $\mathrm{Var}(\mathbf u_i) \approx 1$
:::
::: {.column width="50%"}
$$
\begin{aligned}
\mathbf{x} &= \left(\left(\sqrt{\rho/s}\right)\mathbf{u} + \left(\sqrt{1 - \rho}\right) \mathbf{v} \right)\sigma \\
\mathbf{u} &\sim \mathrm{ICAR}(1) \\
\mathbf{v} &\sim N(\mathbf{0}, n)
\end{aligned}
$$
:::
:::

:::

## Spatial Modeling on Parameter-level

::: {.columns style="font-size:50%"}
### Generalized Extreme Value (GEV) Distribution

::: {.column width="50%"}

* Parameters: 
  - $\mu$: location parameter
    - $\mu = \mu_0 \left(1 + \Delta \left(t - t_0\right)\right)$
    - Model $\mu_0$ and trend $\Delta$
  - $\sigma$: scale parameter
  - $\xi$: shape parameter
$$
\begin{aligned}
\log(\mu_0) = \psi &\sim \mathrm{BYM2}(\mu_{\mu_0}, \rho_{\mu_0}, \sigma_{\mu_0}) \\
\log(\mu_0) - \log(\sigma) = \tau &\sim \mathrm{BYM2}(\mu_\sigma, \rho_\sigma, \sigma_\sigma) \\
f_\xi(\xi) = \phi &\sim \mathrm{BYM2}(\mu_\xi, \rho_\xi, \sigma_\xi) \\
f_\Delta(\Delta) = \gamma &\sim \mathrm{BYM2}(\mu_\Delta, \rho_\Delta, \sigma_\Delta)
\end{aligned}
$$

:::

::: {.column width="50%"}
![](images/facet_constrained.png)
:::

:::

## From Parameter-level to Data-level Dependence

- Limitations of GMRF models for data-level spatial dependence
- Need for new approaches in modeling extreme values
- Introduction to GMRF copulas for data-level dependence
- Our research: Combining GEV distributions with GMRF copulas

## The Need for Copulas in Spatial Extreme Value Modeling

- Limitations of traditional approaches for extreme values
- Why extreme values require special treatment
- Benefits of copulas for modeling dependencies in extremes
  - Flexibility in capturing complex dependence structures
  - Ability to separate marginal distributions from dependence structure
- Specific advantages for spatially distributed extreme values

## Our Approach: GMRF Copulas for GEV Distributions

- Combining GEV distributions with GMRF copulas
- Ensuring unit marginal variance: challenges and solutions
- Why unit marginal variance is crucial for copula modeling
- How this approach addresses the specific needs of spatial extreme value modeling

## Computational Methods {data-name="Methods"}

$$
\mathbf{Q} = \left( \mathbf{Q}_{\rho_1} \otimes \mathbf{I_{n_2}} + \mathbf{I_{n_1}} \otimes \mathbf{Q}_{\rho_2}\right)^{\nu+1}
$$

::: {.columns}
::: {.column width="50%"}
What we want

$$
\boldsymbol \Sigma_{ii} = \left(\mathbf Q^{-1} \right)_{ii} =  1
$$

:::

::: {.column width="50%"}

What we get

$$
\begin{aligned}
\mathbf{\widetilde  Q} &= \mathbf{D}\mathbf{Q}\mathbf{D} \\
\mathbf D_{ii} &= \sqrt{\boldsymbol \Sigma_{ii}}
\end{aligned}
$$

:::

::: 





## Eigendecomposition

:::: {.columns style="font-size:80%"}
We know that

::: {.column width="50%"}


$$
\mathbf{Q}_{\rho_1} = \mathbf{V_{\rho_1}}\boldsymbol \Lambda_{\rho_1}\mathbf{V_{\rho_1}}^T
$$

:::

::: {.column width="50%"}


$$
\mathbf{Q}_{\rho_2} = \mathbf{V_{\rho_2}}\boldsymbol \Lambda_{\rho_2}\mathbf{V_{\rho_2}}^T
$$

:::

Which gives us

$$
\mathbf{Q} = (\mathbf{V_{\rho_1}} \otimes \mathbf{V_{\rho_2}})(\boldsymbol \Lambda_{\rho_1} \otimes \mathbf{I} + \mathbf{I} \otimes \boldsymbol \Lambda_{\rho_2})^{\nu + 1}(\mathbf{V_{\rho_1}} \otimes \mathbf{V_{\rho_2}})^T
$$

::::


::: {.columns style="font-size:80%"}

Efficiently loop over value/vector pairs of smaller matrices

::: {.column width="50%"}

$$
\left\{\lambda_{\rho_1}\right\}_i + \left\{\lambda_{\rho_2}\right\}_j
$$

:::
::: {.column widht="50%"}

$$
\left\{\mathbf{v}_{\rho_1}\right\}_i \otimes \left\{\mathbf{v}_{\rho_2}\right\}_j
$$

:::
:::

## Marginal Standard Deviations

::: {style="font-size:70%"}

$$
\boldsymbol \Sigma = \mathbf Q^{-1} = (\mathbf{V}\boldsymbol\Lambda\mathbf{V}^T)^{-1} = \mathbf{V}\boldsymbol \Lambda^{-1}\mathbf{V}
$$

We know that if $A = BC$ then $A_{ii} = B_{i, .} C_{., i}$, so

$$
\boldsymbol \Sigma_{ii} = \sum_{k=1}^{n} v_{ik} \frac{1}{\lambda_k} (v^T)_{ki} = \sum_{k=1}^{n} v_{ik} \frac{1}{\lambda_k} v_{ik} = \sum_{k=1}^{n} v_{ik}^2 \frac{1}{\lambda_k}
$$

Compute vector $\sigma^2$ containing all marginal variances

$$ 
\sigma^2 = \sum_{i = 1}^{n_1} \sum_{j=1}^{n_2} \frac{\left(\left\{\mathbf{v}_{\rho_1}\right\}_i \otimes \left\{\mathbf{v}_{\rho_2}\right\}_j\right)^{2}}{\quad\left(\left\{\lambda_{\rho_1}\right\}_i + \left\{\lambda_{\rho_2}\right\}_j\right)^{\nu+1}}
$$

:::

## Marginal Standard Deviations

:::: {.columns style="font-size:75%"}
::: {.column width="58%"}
```{r}
#| echo: true
dim1 <- 50; dim2 <- 50
rho1 <- 0.5; rho2 <- 0.3
nu <- 2

Q1 <- make_AR_prec_matrix(dim1, rho1)
Q2 <- make_AR_prec_matrix(dim2, rho2)

I1 <- Matrix::Diagonal(dim1)
I2 <- Matrix::Diagonal(dim2)

Q <- temp <- kronecker(Q1, I2) + kronecker(I1, Q2)
for (i in seq_len(nu)) Q <- Q %*% temp
```
:::

::: {.column width="42%"}
```{r}
#| echo: true
msd <- function(Q1, Q2) {

  E1 <- eigen(Q1)
  E2 <- eigen(Q2)

  marginal_sd_eigen(
    E1$values, E1$vectors, dim1,
    E2$values, E2$vectors, dim2,
    nu
  ) |> 
  sort()
}
```
:::
:::: 

::: {style="font-size:75%"}
```{r}
#| echo: true
#| cache: true
bench::mark(
  "solve" = solve(Q) |> diag() |> sqrt() |> sort(),
  "inla.qinv" = inla.qinv(Q) |> diag() |> sqrt() |> sort(),
  "marginal_sd_eigen" = msd(Q1, Q2),
  iterations = 10,
  filter_gc = FALSE
)
```
:::

## Calculating the (unscaled) density

::: {style="font-size:70%"}

The Gaussian copula log pdf is
$$
\log c(\mathbf{u}) = \frac{1}{2}\log|\mathbf{Q}| - \frac{1}{2}\mathbf{z}^T\mathbf{Q}\mathbf{z} + \frac{1}{2}\mathbf{z}^T\mathbf{z}
$$

Without scaling of $\mathbf Q$ we get

$$
\log|\mathbf{Q}| = \sum_{k=1}^{n_1n_2}\log\lambda_k = \sum_{i=1}^{n_1}\sum_{j=2}^{n_2} \log\left[\left(\left\{\lambda_{\rho_1}\right\}_i + \left\{\lambda_{\rho_2}\right\}_j\right)^{\nu + 1}\right]
$$

$$
\mathbf{z}^T\mathbf{Q}\mathbf{z} = \sum_{k=1}^{n_1n_2}\lambda_k \left(v_k^T\mathbf z\right)^2 = 
\sum_{i=1}^{n_1}\sum_{j=2}^{n_2} 
\left(\left\{\lambda_{\rho_1}\right\}_i + \left\{\lambda_{\rho_2}\right\}_j\right)
\left[\left(\left\{\mathbf{v}_{\rho_1}\right\}_i \otimes \left\{\mathbf{v}_{\rho_2}\right\}_j\right)^T\mathbf z\right]^2
$$

:::

## Calculating the copula density

::: {style="font-size:70%"}

Let $\mathbf D_{ii} = \sigma_i$, $\mathbf v = \left\{\mathbf{v}_{\rho_1}\right\}_i \otimes \left\{\mathbf{v}_{\rho_2}\right\}_j$ and $\lambda = \left(\left\{\lambda_{\rho_1}\right\}_i + \left\{\lambda_{\rho_2}\right\}_j\right)^{\nu + 1}$. Normalise $\mathbf v$ and $\lambda$ with

$$
\begin{gathered}
\widetilde{\mathbf{v}} = \frac{D\mathbf{v}}{\vert\vert D\mathbf{v}\vert\vert_2}, \qquad
\widetilde{\lambda} = \vert\vert D\mathbf{v}\vert\vert_2^2 \cdot \lambda
\end{gathered}
$$

Then $\widetilde{v}$ and $\widetilde{\lambda}$ are an eigenvector/value pair of the scaled precision matrix $\mathbf{\widetilde{Q}}$. Iterate over $i$ and $j$ to calculate

$$
\log c(\mathbf{u}) = \frac{1}{2}\log|\mathbf{\widetilde Q}| - \frac{1}{2}\mathbf{z}^T\mathbf{\widetilde Q}\mathbf{z} + \frac{1}{2}\mathbf{z}^T\mathbf{z}
$$

:::

## Folded Circulant Approximation

::: {style="font-size:40%"}

::: {.columns}
::: {.column width="40%"}
### AR(1) precision

The exact form of $Q_{\rho}$, the precision matrix of a one-dimensional AR(1) process with correlation $\rho$ is

:::
::: {.column width="60%"}
$$
\mathbf{Q}_\rho = \frac{1}{1-\rho^2}
\begin{bmatrix}
1 & -\rho & 0 & \cdots & 0 \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$
:::
::: 

----------




::: {.columns}
::: {.column width="40%"}
### Circulant Approximation

This approximation treats the first and last observations as neighbors, effectively wrapping the data around a circle.

:::
::: {.column width="60%"}
$$
\mathbf{Q}_\rho^{(circ)} = \frac{1}{1-\rho^2}
\begin{bmatrix}
1+\rho^2 & -\rho & 0 & \cdots & 0 & -\rho \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
-\rho & 0 & 0 & \cdots & -\rho & 1+\rho^2
\end{bmatrix}
$$
:::
::: 

----------


::: {.columns}
::: {.column width="40%"}
### Folded Circulant Approximation

We double the data by reflecting it, giving us the data $x_1,  \dots, x_n, x_n, \dots, x_1$. We then model this doubled data with a $2n \times 2n$ circulant matrix. If written out as an $n \times n$ matrix, it takes the form:

:::
::: {.column width="60%"}
$$
\mathbf{Q}_\rho^{(fold)} = \frac{1}{1-\rho^2}
\begin{bmatrix}
1-\rho+\rho^2 & -\rho & 0 & \cdots & 0 & 0 \\
-\rho & 1+\rho^2 & -\rho & \cdots & 0 & 0 \\
0 & -\rho & 1+\rho^2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & -\rho & 1-\rho+\rho^2
\end{bmatrix}
$$
:::
::: 


:::

## Results and Discussion {data-name="Results"}

- Performance on simulated and real datasets
- Comparison with traditional methods (including BYM2)
- Advantages in interpretability and scalability
- Improved modeling of spatial dependencies in extreme values

## Data Generation


:::: {.columns .r-fit-text style="font-size:70%"}

```{r}
#| echo: true

tictoc::tic()
X <- rmatern_copula_folded_full(n = 100, dim1 = 160, dim2 = 90, rho1 = 0.8, rho2 = 0.9, nu = 2)
tictoc::toc()
```

::: {.column width="50%"}

```{r}
#| echo: true
plot_matern(X[, 1], 160, 90)
```

```{r}
#| echo: true
plot_matern(X[, 2], 160, 90)
```
:::

::: {.column width="50%"}

```{r}
#| echo: true
apply(X, 1, var) |> hist()
```

```{r}
#| echo: true
apply(X, 1, mean) |> hist()
```
:::

::::

## Conclusion and Future Work {data-name="Conclusion"}

- Key contributions of our approach
- Potential applications in various fields (e.g., climate science, hydrology)
- Future research directions

## Thank You

- Contact information: brynjolfur@hi.is
- Website: bggj.is
- Q&A